"""
üåä STREAMING CHAT ROUTES - LIVE RESPONSE STREAMING
=================================================

Real-time streaming chat responses with:
- Server-Sent Events (SSE) for live typing effect
- Source citations with expandable details
- AI reasoning process visualization
- Live progress updates

The most engaging agricultural chat experience! üöÄ‚ú®
"""

import asyncio
import json
import logging
from typing import AsyncGenerator, Dict, Any
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import update

from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.models.user import User
from app.models.chat import ChatSession, ChatMessage
from app.schemas.chat import ChatMessageCreate, MessageRole
from app.services.chat_service import chat_service

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create router
router = APIRouter(prefix="/streaming", tags=["üåä Live Streaming Chat"])

class StreamingChatService:
    """Service for streaming chat responses with live updates"""
    
    @staticmethod
    async def stream_response(
        db: AsyncSession,
        user_id: str,
        message_data: ChatMessageCreate
    ) -> AsyncGenerator[str, None]:
        """Stream chat response with live updates using REAL AI pipeline"""
        
        try:
            # Import the real AI services
            from app.tools.rag_core.simple_rag_orchestrator import process_agricultural_query
            from app.tools.fact_checker.agricultural_fact_checker import agricultural_fact_checker
            
            # Step 1: Send initial status
            yield f"data: {json.dumps({'type': 'status', 'message': 'üîç Processing your agricultural query...', 'progress': 10})}\n\n"
            await asyncio.sleep(0.1)
            
            # Verify session exists and belongs to user
            session = await chat_service.get_session(db, message_data.session_id, user_id)
            if not session:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Session not found or access denied'})}\n\n"
                return
            
            # Step 2: Language detection
            yield f"data: {json.dumps({'type': 'phase', 'phase': 'language_detection', 'title': 'üåê Language Detection', 'status': 'processing'})}\n\n"
            await asyncio.sleep(0.3)
            
            # Get conversation history for context
            chat_history = await chat_service._get_chat_history(db, message_data.session_id)
            
            # Enhance query with conversation context
            enhanced_query = chat_service._enhance_query_with_context(
                message_data.content, 
                chat_history, 
                session.language_preference
            )
            
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 'language_detection', 'result': f'Language: {session.language_preference}', 'progress': 25})}\n\n"
            
            # Step 3: Agricultural intelligence processing
            yield f"data: {json.dumps({'type': 'phase', 'phase': 'ai_processing', 'title': 'üß† Agricultural AI Processing', 'status': 'processing'})}\n\n"
            await asyncio.sleep(0.5)
            
            # Show what's being analyzed
            yield f"data: {json.dumps({'type': 'ai_step', 'step': 'Analyzing agricultural query context', 'detail': 'Understanding farming context and requirements'})}\n\n"
            await asyncio.sleep(0.3)
            
            yield f"data: {json.dumps({'type': 'ai_step', 'step': 'Searching knowledge base', 'detail': 'Finding relevant agricultural information'})}\n\n"
            await asyncio.sleep(0.3)
            
            yield f"data: {json.dumps({'type': 'ai_step', 'step': 'Integrating weather and market data', 'detail': 'Getting real-time agricultural intelligence'})}\n\n"
            await asyncio.sleep(0.3)
            
            # Process with REAL RAG system
            ai_response = await process_agricultural_query(enhanced_query)
            
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 'ai_processing', 'result': 'AI analysis complete', 'progress': 60})}\n\n"
            
            # Step 4: Fact checking with the REAL fact checker
            yield f"data: {json.dumps({'type': 'phase', 'phase': 'fact_check', 'title': '‚úÖ Agricultural Fact Checking', 'status': 'processing'})}\n\n"
            await asyncio.sleep(0.3)
            
            # Use the REAL fact checker
            fact_check_result = await agricultural_fact_checker.validate_and_respond(
                original_query=message_data.content,
                expert_response=ai_response.get('response', ''),
                context_data={
                    'weather_intelligence': ai_response.get('weather_data'),
                    'search_results': ai_response.get('search_results', []),
                    'agricultural_data': ai_response.get('agricultural_data'),
                    'market_intelligence': ai_response.get('market_data')
                }
            )
            
            # Stream fact check process
            yield f"data: {json.dumps({'type': 'fact_check_step', 'step': 'Validating agricultural accuracy', 'status': 'checking'})}\n\n"
            await asyncio.sleep(0.2)
            
            yield f"data: {json.dumps({'type': 'fact_check_step', 'step': 'Verifying safety recommendations', 'status': 'checking'})}\n\n"
            await asyncio.sleep(0.2)
            
            yield f"data: {json.dumps({'type': 'fact_check_step', 'step': 'Confirming regional relevance', 'status': 'checking'})}\n\n"
            await asyncio.sleep(0.2)
            
            # Get the final verified response
            final_response = fact_check_result.get('final_response', ai_response.get('response', 'Unable to process response'))
            validation_status = fact_check_result.get('validation_status', 'approved')
            
            yield f"data: {json.dumps({'type': 'fact_check_result', 'status': validation_status, 'confidence': fact_check_result.get('fact_check_details', {}).get('confidence', 0.9), 'message': 'Information verified against agricultural databases'})}\n\n"
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 'fact_check', 'result': f'Status: {validation_status}', 'progress': 85})}\n\n"
            
            # Step 5: Stream the REAL FINAL RESPONSE from fact checker
            yield f"data: {json.dumps({'type': 'response_start', 'message': '‚ú® Delivering verified agricultural advice...'})}\n\n"
            await asyncio.sleep(0.2)
            
            # Stream the REAL response word by word for realistic typing effect
            words = final_response.split()
            current_chunk = ""
            
            for i, word in enumerate(words):
                current_chunk += word + " "
                
                # Send chunks every 3-5 words for realistic streaming
                if i % 4 == 0 or i == len(words) - 1:
                    yield f"data: {json.dumps({'type': 'response_chunk', 'chunk': current_chunk, 'progress': 85 + (i / len(words)) * 14})}\n\n"
                    current_chunk = ""
                    await asyncio.sleep(0.1)  # Realistic typing speed
            
            # Step 6: Save messages to database
            yield f"data: {json.dumps({'type': 'phase', 'phase': 'saving', 'title': 'üíæ Saving to database', 'status': 'processing'})}\n\n"
            
            # Create and save user message
            user_message = ChatMessage(
                session_id=message_data.session_id,
                role=MessageRole.USER.value,
                content=message_data.content,
                original_language=session.language_preference
            )
            
            # Create and save AI message with fact-checked response
            ai_message = ChatMessage(
                session_id=message_data.session_id,
                role=MessageRole.ASSISTANT.value,
                content=final_response,
                original_language=fact_check_result.get('original_language', session.language_preference),
                processing_time=ai_response.get('processing_time', 0.0),
                fact_check_status=validation_status,
                confidence_score=fact_check_result.get('fact_check_details', {}).get('confidence', 0.9),
                expert_consulted=ai_response.get('expert_consulted', 'general-agriculture'),
                tools_used=ai_response.get('sources_used', [])
            )
            
            # Save to database
            db.add(user_message)
            db.add(ai_message)
            
            # Update session
            from sqlalchemy import update
            await db.execute(
                update(ChatSession)
                .where(ChatSession.id == message_data.session_id)
                .values(
                    updated_at=datetime.utcnow(),
                    message_count=ChatSession.message_count + 2
                )
            )
            await db.commit()
            
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 'saving', 'result': 'Messages saved', 'progress': 99})}\n\n"
            
            # Step 7: Final completion with metadata
            completion_data = {
                'type': 'completion', 
                'message': 'Response generated successfully!', 
                'progress': 100,
                'response_id': f'msg_{ai_message.id}',
                'validation_status': validation_status,
                'confidence': fact_check_result.get('fact_check_details', {}).get('confidence', 0.9),
                'language': fact_check_result.get('original_language', session.language_preference),
                'sources_used': ai_response.get('sources_used', []),
                'fact_checker_used': fact_check_result.get('processing_info', {}).get('fact_checker_used', True)
            }
            yield f"data: {json.dumps(completion_data)}\n\n"
            
        except Exception as e:
            logger.error(f"‚ùå Streaming error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': f'Processing failed: {str(e)}'})}\n\n"

@router.post("/chat")
async def stream_chat_response(
    message_data: ChatMessageCreate,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    üåä STREAM CHAT RESPONSE
    
    Returns a Server-Sent Events stream with:
    - Live typing effect
    - Processing phases with progress
    - Source citations
    - AI reasoning steps
    - Real-time updates
    """
    
    async def generate():
        async for chunk in StreamingChatService.stream_response(
            db=db,
            user_id=str(current_user.id),
            message_data=message_data
        ):
            # Check if client disconnected
            if await request.is_disconnected():
                logger.info("üîå Client disconnected, stopping stream")
                break
            yield chunk
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

@router.get("/chat")
async def stream_chat_response_get(
    session_id: str,
    content: str,
    token: str,
    request: Request,
    db: AsyncSession = Depends(get_db)
):
    """
    üåä STREAM CHAT RESPONSE (GET for EventSource)
    
    EventSource-compatible endpoint that streams chat responses
    """
    try:
        # Verify token and get user
        from app.core.dependencies import get_current_user_from_token
        current_user = await get_current_user_from_token(token, db)
        
        # Create message data
        message_data = ChatMessageCreate(
            session_id=session_id,
            content=content
        )
        
        # Create streaming generator
        async def generate_stream():
            async for chunk in StreamingChatService.stream_response(
                db=db,
                user_id=str(current_user.id),
                message_data=message_data
            ):
                yield chunk
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
        
    except Exception as e:
        logger.error(f"‚ùå Streaming GET error: {e}")
        return HTTPException(status_code=500, detail=str(e))

@router.get("/health")
async def streaming_health():
    """Health check for streaming service"""
    return {
        "status": "healthy",
        "service": "Streaming Chat",
        "features": [
            "Server-Sent Events",
            "Live response streaming",
            "Source citations",
            "AI reasoning visualization",
            "Progress tracking"
        ]
    }
